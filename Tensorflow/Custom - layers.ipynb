{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* layer : 유용한 연산자의 집합 \n",
    "    \n",
    "* 케라스에 미리 구성되어 잇는 층 \n",
    "    - dense : 완전 연결 층 \n",
    "    - Conv2D\n",
    "    - LSTM\n",
    "    - BatchNormalication \n",
    "    - Dropout \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers 패키지에서 층은 객체입니다. 층을 구성하려면 간단히 객체를 생성하십시오.\n",
    "# 대부분의 layer는 첫번째 인수로 출력 차원(크기) 또는 채널을 취합니다.\n",
    "layer = tf.keras.layers.Dense(100)\n",
    "\n",
    "# 입력 차원의 수는 층을 처음 실행할 때 유추할 수 있기 때문에 종종 불필요합니다. \n",
    "# 일부 복잡한 모델에서는 수동으로 입력 차원의 수를 제공하는것이 유용할 수 있습니다.\n",
    "\n",
    "# 10은 출력을 나타냄 \n",
    "\n",
    "layer = tf.keras.layers.Dense(10, input_shape=(None, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=269, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 층을 사용하려면, 호출 해야함\n",
    "\n",
    "layer(tf.zeros([10,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_15/kernel:0' shape=(7, 10) dtype=float32, numpy=\n",
       " array([[-0.24468443,  0.36570823, -0.50602806,  0.0409928 ,  0.25654352,\n",
       "         -0.17692274,  0.36559206, -0.28704438, -0.4708591 , -0.1436069 ],\n",
       "        [ 0.3964762 , -0.06430972,  0.10296065,  0.3344503 , -0.40664467,\n",
       "          0.20018202, -0.32130313,  0.05540627, -0.43519837, -0.05430573],\n",
       "        [ 0.5062207 ,  0.4942273 , -0.47166973,  0.49318624,  0.23126596,\n",
       "          0.24378711, -0.1921233 , -0.38863578, -0.36614293, -0.3132553 ],\n",
       "        [-0.39328545, -0.23518479,  0.21211094,  0.4652133 ,  0.5241786 ,\n",
       "         -0.06512135,  0.0276795 ,  0.4341718 , -0.05079955,  0.36884207],\n",
       "        [-0.06920916, -0.38888633, -0.12757474, -0.4586153 , -0.01599133,\n",
       "          0.27863252, -0.47755876,  0.03030252, -0.50509095,  0.19988269],\n",
       "        [-0.13910961,  0.4225527 , -0.06560522, -0.40528208, -0.39774817,\n",
       "         -0.3917155 , -0.35198683, -0.22009939,  0.13298136,  0.02105629],\n",
       "        [-0.5216981 ,  0.33309847,  0.12213284,  0.31052107, -0.23354656,\n",
       "         -0.39706686, -0.3096593 ,  0.37076062, -0.5672106 ,  0.3713284 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_15/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer는 유용한 메서드를 많이 가지고 있음 \n",
    " # ex) layer.variables : 층안에 있는 모든 변수를 확인 \n",
    "\n",
    "# 완전연결층 (Dense)층은 가중치와 편향을 위한 변수를 가짐 \n",
    "\n",
    "layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_9/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수는 객체의 속성을 통해 편리하게 접근 가능 \n",
    "\n",
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n",
      "5 10\n",
      "<tf.Variable 'my_dense_layer_8/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[ 0.36545092,  0.07518017, -0.2178407 ,  0.16378689, -0.11743248,\n",
      "        -0.06988668, -0.1886259 , -0.3557327 ,  0.01777667,  0.14509684],\n",
      "       [-0.20203093, -0.6170791 , -0.5451358 ,  0.3793344 , -0.6084333 ,\n",
      "        -0.13391978,  0.42714995,  0.4112684 , -0.57350755, -0.23149347],\n",
      "       [ 0.35965234,  0.23762834,  0.11156452, -0.3158066 ,  0.49690932,\n",
      "         0.22180212,  0.3438124 , -0.30851158,  0.51698107,  0.14362997],\n",
      "       [-0.4153428 ,  0.03042698, -0.09771818,  0.01686412,  0.20391762,\n",
      "         0.07340413,  0.43899232,  0.07386935,  0.5109635 , -0.32636908],\n",
      "       [ 0.2731306 ,  0.06507546, -0.09583074,  0.5048345 ,  0.09806168,\n",
      "         0.29162621, -0.22845295, -0.5382868 ,  0.1399355 ,  0.0825358 ]],\n",
      "      dtype=float32)>\n",
      "(10, 5)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer_8/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[ 0.36545092,  0.07518017, -0.2178407 ,  0.16378689, -0.11743248,\n",
      "        -0.06988668, -0.1886259 , -0.3557327 ,  0.01777667,  0.14509684],\n",
      "       [-0.20203093, -0.6170791 , -0.5451358 ,  0.3793344 , -0.6084333 ,\n",
      "        -0.13391978,  0.42714995,  0.4112684 , -0.57350755, -0.23149347],\n",
      "       [ 0.35965234,  0.23762834,  0.11156452, -0.3158066 ,  0.49690932,\n",
      "         0.22180212,  0.3438124 , -0.30851158,  0.51698107,  0.14362997],\n",
      "       [-0.4153428 ,  0.03042698, -0.09771818,  0.01686412,  0.20391762,\n",
      "         0.07340413,  0.43899232,  0.07386935,  0.5109635 , -0.32636908],\n",
      "       [ 0.2731306 ,  0.06507546, -0.09583074,  0.5048345 ,  0.09806168,\n",
      "         0.29162621, -0.22845295, -0.5382868 ,  0.1399355 ,  0.0825358 ]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 층 구현하기 \n",
    "\n",
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # __init__ : 층에 필요한 매개변수를 입력 받음 \n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "    # build : 입력 텐서의 크기를 알고 나머지를 초기화 할 수 있음 \n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        self.kernel = self.add_variable(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
    "        print(input_shape[-1], self.num_outputs)\n",
    "        print(layer.kernel)\n",
    "        print(input_shape)\n",
    "    \n",
    "    # call : 정방향 연산(forward computation) 진행 가능 \n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "    \n",
    "\n",
    "layer = MyDenseLayer(10)\n",
    "print(layer(tf.zeros([10,5])))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block_1/conv2d_6/kernel:0', 'resnet_identity_block_1/conv2d_6/bias:0', 'resnet_identity_block_1/batch_normalization_v1_6/gamma:0', 'resnet_identity_block_1/batch_normalization_v1_6/beta:0', 'resnet_identity_block_1/conv2d_7/kernel:0', 'resnet_identity_block_1/conv2d_7/bias:0', 'resnet_identity_block_1/batch_normalization_v1_7/gamma:0', 'resnet_identity_block_1/batch_normalization_v1_7/beta:0', 'resnet_identity_block_1/conv2d_8/kernel:0', 'resnet_identity_block_1/conv2d_8/bias:0', 'resnet_identity_block_1/batch_normalization_v1_8/gamma:0', 'resnet_identity_block_1/batch_normalization_v1_8/beta:0']\n"
     ]
    }
   ],
   "source": [
    "# 머신러닝 모델에의 많은 것은 기존 층을 조합하여 구현된다. \n",
    "\n",
    "# 레스넷(resnet)의 각 잔여 블록(residual block)은 합성곱(convolution), 배치 정규화(batch normalization), 쇼트컷(shortcut) 등으로 구성\n",
    "\n",
    "# 다른층을 포함한 모델을 만들기 위해 사용하는 메인 클래스는 tf.keras.Model 이다. \n",
    "# tf.keras.Model을 상송하여 구현해보자 \n",
    "\n",
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    # https://tykimos.tistory.com/12 참고\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x += input_tensor\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "    \n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=914, shape=(1, 2, 3, 3), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그러나 대부분의 경우에, 많은 층으로 구성된 모델은 간단하게 연이어 하나의 층으로 호출할 수 있습니다. 이는 tf.keras.Sequential 사용하여 간단한 코드로 구현 가능\n",
    "\n",
    "my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1)),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                               tf.keras.layers.Conv2D(2, 1, \n",
    "                                                      padding='same'),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                               tf.keras.layers.Conv2D(3, (1, 1)),\n",
    "                               tf.keras.layers.BatchNormalization()])\n",
    "my_seq(tf.zeros([1, 2, 3, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
