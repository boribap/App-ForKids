{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "[[[[ 70.]\n",
      "   [ 80.]\n",
      "   [ 82.]\n",
      "   ...\n",
      "   [ 52.]\n",
      "   [ 43.]\n",
      "   [ 41.]]\n",
      "\n",
      "  [[ 65.]\n",
      "   [ 61.]\n",
      "   [ 58.]\n",
      "   ...\n",
      "   [ 56.]\n",
      "   [ 52.]\n",
      "   [ 44.]]\n",
      "\n",
      "  [[ 50.]\n",
      "   [ 43.]\n",
      "   [ 54.]\n",
      "   ...\n",
      "   [ 49.]\n",
      "   [ 56.]\n",
      "   [ 47.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 91.]\n",
      "   [ 65.]\n",
      "   [ 42.]\n",
      "   ...\n",
      "   [ 72.]\n",
      "   [ 56.]\n",
      "   [ 43.]]\n",
      "\n",
      "  [[ 77.]\n",
      "   [ 82.]\n",
      "   [ 79.]\n",
      "   ...\n",
      "   [105.]\n",
      "   [ 70.]\n",
      "   [ 46.]]\n",
      "\n",
      "  [[ 77.]\n",
      "   [ 72.]\n",
      "   [ 84.]\n",
      "   ...\n",
      "   [106.]\n",
      "   [109.]\n",
      "   [ 82.]]]]\n",
      "[[1]]\n",
      "[[[[-0.45098037]\n",
      "   [-0.372549  ]\n",
      "   [-0.35686272]\n",
      "   ...\n",
      "   [-0.5921569 ]\n",
      "   [-0.6627451 ]\n",
      "   [-0.6784314 ]]\n",
      "\n",
      "  [[-0.49019605]\n",
      "   [-0.52156866]\n",
      "   [-0.54509807]\n",
      "   ...\n",
      "   [-0.56078434]\n",
      "   [-0.5921569 ]\n",
      "   [-0.654902  ]]\n",
      "\n",
      "  [[-0.60784316]\n",
      "   [-0.6627451 ]\n",
      "   [-0.5764706 ]\n",
      "   ...\n",
      "   [-0.6156863 ]\n",
      "   [-0.56078434]\n",
      "   [-0.6313726 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.2862745 ]\n",
      "   [-0.49019605]\n",
      "   [-0.67058825]\n",
      "   ...\n",
      "   [-0.4352941 ]\n",
      "   [-0.56078434]\n",
      "   [-0.6627451 ]]\n",
      "\n",
      "  [[-0.3960784 ]\n",
      "   [-0.35686272]\n",
      "   [-0.38039213]\n",
      "   ...\n",
      "   [-0.17647058]\n",
      "   [-0.45098037]\n",
      "   [-0.6392157 ]]\n",
      "\n",
      "  [[-0.3960784 ]\n",
      "   [-0.4352941 ]\n",
      "   [-0.34117645]\n",
      "   ...\n",
      "   [-0.16862744]\n",
      "   [-0.14509803]\n",
      "   [-0.35686272]]]]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기 \n",
    "\n",
    "dataset_path = './fer2013test.csv'\n",
    "image_size=(48,48)\n",
    " \n",
    "def load_fer2013():\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        print(len(face))\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'),image_size)\n",
    "        faces.append(face.astype('float32'))\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "    return faces, emotions\n",
    " \n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    " \n",
    "faces, emotions = load_fer2013()\n",
    "print(faces)\n",
    "print(emotions)\n",
    "faces = preprocess_input(faces)\n",
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 48, 48, 1) float32\n",
      "(1, 48, 48, 1) float32\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터 셋과 테스트 데이터 셋 나누기\n",
    "\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)\n",
    "print(xtrain.shape, xtrain.dtype)\n",
    "print(xtest.shape, xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용하는 파라미터 정리 \n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 110\n",
    "input_shape = (48,48,1)\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = './'\n",
    "l2_regularization=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "regularization = l2(l2_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "img_input = Input(input_shape)\n",
    "x = Conv2D(8,(3,3), strides=(1,1), kernel_regularizer=regularization, use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(8,(3,3), strides=(1,1), kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module 1\n",
    "\n",
    "residual = Conv2D(16,(1,1), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(16,(3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(16,(3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3,3), strides=(2,2), padding='same')(x)\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module 2\n",
    "\n",
    "residual = Conv2D(32, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module 3\n",
    "residual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"global_average_pooling2d_1/Mean:0\", shape=(?, 7), dtype=float32)\n",
      "(?, 7)\n"
     ]
    }
   ],
   "source": [
    "# module 4\n",
    "\n",
    "residual = Conv2D(128,(1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = Conv2D(num_classes, (3,3), padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "print(x)\n",
    "output = Activation('softmax', name='predictions')(x)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 46, 46, 8)    72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 46, 46, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 46, 46, 8)    0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 44, 44, 8)    576         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 44, 44, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 44, 44, 8)    0           batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 44, 44, 16)   200         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 44, 44, 16)   64          separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 44, 44, 16)   0           batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   400         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 22, 22, 16)   128         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 22, 22, 16)   0           batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 22, 22, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 22, 22, 16)   0           max_pooling2d[0][0]              \n",
      "                                                                 batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 22, 22, 32)   656         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 22, 22, 32)   128         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 22, 22, 32)   0           batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   1312        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 11, 32)   512         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 11, 11, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 11, 11, 64)   2336        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 11, 11, 64)   256         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 11, 11, 64)   0           batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   4672        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 6, 6, 64)     2048        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 6, 6, 64)     256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 6, 6, 128)    8768        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 6, 6, 128)    512         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 6, 6, 128)    0           batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    17536       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 3, 128)    8192        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 3, 3, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_v1_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 3, 3, 7)      8071        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 7)            0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 58,423\n",
      "Trainable params: 56,951\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model \n",
    "\n",
    "model = Model(img_input, output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "log_file_path = base_path + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1, save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/110\n",
      "7178/7178 [==============================] - 38s 5ms/sample - loss: 1.4605 - acc: 0.4678\n",
      "\n",
      "Epoch 00110: val_loss improved from 1.63602 to 1.46092, saving model to ./_mini_XCEPTION.110-0.467818.hdf5\n",
      "898/898 [==============================] - 1481s 2s/step - loss: 1.5274 - acc: 0.4292 - val_loss: 1.4609 - val_acc: 0.4678\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작 \n",
    "\n",
    "xecption_history = model.fit_generator(data_generator.flow(xtrain, ytrain, batch_size),\n",
    "                       steps_per_epoch=len(xtrain) / batch_size,\n",
    "                       epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                       validation_data=(xtest, ytest),initial_epoch=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:352: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-820afd7cab68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_face'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_output/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morig_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.0) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:352: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    " \n",
    "# parameters for loading data and images\n",
    "detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = './_mini_XCEPTION.110-0.471998.hdf5'\n",
    "img_path = sys.argv[1]\n",
    " \n",
    "# hyper-parameters for bounding boxes shape\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\",\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]\n",
    " \n",
    "#reading the frame\n",
    "orig_frame = cv2.imread(img_path)\n",
    "frame = cv2.imread(img_path,0)\n",
    "faces = face_detection.detectMultiScale(frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    " \n",
    "if len(faces) > 0:\n",
    "    faces = sorted(faces, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
    "    (fX, fY, fW, fH) = faces\n",
    "    roi = frame[fY:fY + fH, fX:fX + fW]\n",
    "    roi = cv2.resize(roi, (48, 48))\n",
    "    roi = roi.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    preds = emotion_classifier.predict(roi)[0]\n",
    "    emotion_probability = np.max(preds)\n",
    "    label = EMOTIONS[preds.argmax()]\n",
    "    cv2.putText(orig_frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    cv2.rectangle(orig_frame, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    " \n",
    "\n",
    "cv2.imshow('test_face', orig_frame)\n",
    "cv2.imwrite('test_output/'+img_path.split('/')[-1],orig_frame)\n",
    "if (cv2.waitKey(2000) & 0xFF == ord('q')):\n",
    "    sys.exit(\"Thanks\")\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 손실과 검증 손실 그래프 그리기 \n",
    "\n",
    "## 실선 - 훈련손실 \n",
    "## 점선 - 검증 손실 \n",
    "    ### --> 낮은 검증 손실이 더 좋은 모델 \n",
    "    \n",
    "    \n",
    "print(xecption_history.history.keys())\n",
    "    \n",
    "\n",
    "def plot_history(histories, key='loss'):\n",
    "  plt.figure(figsize=(16,10))\n",
    "    \n",
    "  for name, history in histories:\n",
    "    val = plt.plot(history.epoch, history.history['lr'],\n",
    "                   '--', label=name.title()+' Val')\n",
    "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel(key.replace('_',' ').title())\n",
    "  plt.legend()\n",
    "\n",
    "  plt.xlim([0,max(history.epoch)])\n",
    "\n",
    "\n",
    "plot_history([('xecption', xecption_history)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
